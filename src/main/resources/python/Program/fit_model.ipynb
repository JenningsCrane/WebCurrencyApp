{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ae55cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# граница тренировочных и валидационных данных\n",
    "TRAIN_SPLIT = 0\n",
    "# кол-во обучающих данных\n",
    "EVALUATION_INTERVAL = 0\n",
    "# кол-во эпох обучения\n",
    "EPOCHS = 10\n",
    "# настройки для обучения\n",
    "BATCH_SIZE = 256\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# Функция подготовки данных для входа в рекурентную сеть\n",
    "def univariate_data(dataset, start_index, end_index, history_size,\n",
    "                      target_size, step, single_step=False):\n",
    "  data = []\n",
    "  labels = []\n",
    "\n",
    "  start_index = start_index + history_size\n",
    "  if end_index is None:\n",
    "    end_index = len(dataset) - target_size\n",
    "\n",
    "  for i in range(start_index, end_index):\n",
    "    indices = range(i-history_size, i, step)\n",
    "    data.append(np.reshape(dataset[indices], (history_size, 1)))\n",
    "\n",
    "    labels.append(dataset[i:i+target_size])\n",
    "\n",
    "  return np.array(data), np.array(labels)\n",
    "\n",
    "def load_model_from_keras_format(directory):\n",
    "    model = tf.keras.models.load_model(directory)\n",
    "    return model\n",
    "\n",
    "def fine_tune_model(model, train_data, val_data):\n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(clipvalue=1.0), loss='mae')\n",
    "    model.fit(train_data, epochs=EPOCHS,\n",
    "                                          steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                          validation_data=val_data,\n",
    "                                          validation_steps=50)\n",
    "    return model\n",
    "\n",
    "def check_csv_file_size(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        num_records = len(df)\n",
    "        if (num_records >= 50):\n",
    "            TRAIN_SPLIT = int(num_records * 0.9)\n",
    "            EVALUATION_INTERVAL = num_records - 40\n",
    "            return num_records\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Путь к директории с моделью\n",
    "directory = \"..\\\\model\\\\model.keras\"\n",
    "\n",
    "# Путь к файлу CSV\n",
    "csv_file_path = \"..\\\\new_dataset\\\\uans.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f02366a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp\\ipykernel_11780\\1350198284.py:24: RuntimeWarning: Mean of empty slice.\n",
      "  uni_train_mean = uni_data[:TRAIN_SPLIT].mean()\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"Cast:0\", shape=(None,), dtype=float32). Expected shape (None, 30, 1), but input has incompatible shape (None,)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None,), dtype=float32)\n  • training=True\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 48\u001b[0m\n\u001b[0;32m     44\u001b[0m     val_data \u001b[38;5;241m=\u001b[39m val_data\u001b[38;5;241m.\u001b[39mbatch(BATCH_SIZE)\u001b[38;5;241m.\u001b[39mrepeat()\n\u001b[0;32m     47\u001b[0m     model \u001b[38;5;241m=\u001b[39m load_model_from_keras_format(directory)\n\u001b[1;32m---> 48\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mfine_tune_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mФайл .keras не найден в указанной директории.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 46\u001b[0m, in \u001b[0;36mfine_tune_model\u001b[1;34m(model, train_data, val_data)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfine_tune_model\u001b[39m(model, train_data, val_data):\n\u001b[0;32m     45\u001b[0m     model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mRMSprop(clipvalue\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEVALUATION_INTERVAL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\models\\functional.py:280\u001b[0m, in \u001b[0;36mFunctional._adjust_input_rank\u001b[1;34m(self, flat_inputs)\u001b[0m\n\u001b[0;32m    278\u001b[0m             adjusted\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    279\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m     )\n\u001b[0;32m    284\u001b[0m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"Cast:0\", shape=(None,), dtype=float32). Expected shape (None, 30, 1), but input has incompatible shape (None,)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None,), dtype=float32)\n  • training=True\n  • mask=None"
     ]
    }
   ],
   "source": [
    "# Проверка размера файла CSV\n",
    "if check_csv_file_size(csv_file_path):\n",
    "    # Загрузка модели из .keras файла\n",
    "    if os.path.exists(directory):\n",
    "        # Загрузка данных из CSV файла\n",
    "        df = pd.read_csv('..\\\\dataset\\\\uan20years.csv', on_bad_lines='skip', sep=';')\n",
    "\n",
    "        # Преобразование значений столбца 'curs' из строки в число с корректным разделителем\n",
    "        df['curs'] = df['curs'].str.replace(',', '.').astype(float)\n",
    "        # Применение логики для разделения значений столбцов 'nominal' и 'curs'\n",
    "        df['curs'] = df.apply(lambda row: row['curs'] / 10.0 if row['nominal'] == 10 else row['curs'], axis=1)\n",
    "\n",
    "        df['nominal'] = df['nominal'].apply(lambda x: x / 10 if x == 10 else x)\n",
    "\n",
    "        # Переворачивание порядка столбцов, кроме первого\n",
    "        df.iloc[:, 1:] = df.iloc[:, 1:][::-1].values\n",
    "        \n",
    "        # извлекаем курс и индексируем по дате\n",
    "        uni_data = df['curs']\n",
    "        uni_data.index = df['data']\n",
    "        uni_data = uni_data.values\n",
    "        \n",
    "        # нормализация данных\n",
    "        uni_train_mean = uni_data[:TRAIN_SPLIT].mean()\n",
    "        uni_train_std = uni_data[:TRAIN_SPLIT].std()\n",
    "        uni_data = (uni_data-uni_train_mean)/uni_train_std\n",
    "        \n",
    "        past_history = 30\n",
    "        future_target = 7\n",
    "        STEP = 1\n",
    "\n",
    "        x_train_uni, y_train_uni = univariate_data(uni_data, 0,\n",
    "                                                 TRAIN_SPLIT, past_history,\n",
    "                                                 future_target, STEP)\n",
    "        x_val_uni, y_val_uni = univariate_data(uni_data,\n",
    "                                             TRAIN_SPLIT, None, past_history,\n",
    "                                             future_target, STEP)\n",
    "        \n",
    "        # подготовка данных для модели\n",
    "        train_data = tf.data.Dataset.from_tensor_slices((x_train_uni, y_train_uni))\n",
    "        train_data = train_data.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "        val_data = tf.data.Dataset.from_tensor_slices((x_val_uni, y_val_uni))\n",
    "        val_data = val_data.batch(BATCH_SIZE).repeat()\n",
    "        \n",
    "        \n",
    "        model = load_model_from_keras_format(directory)\n",
    "        model = fine_tune_model(model, train_data, val_data)\n",
    "    else:\n",
    "        print(\"Файл .keras не найден в указанной директории.\")\n",
    "else:\n",
    "    print(\"Размер файла CSV меньше 50 записей. Переобучение не требуется.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df951d29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
